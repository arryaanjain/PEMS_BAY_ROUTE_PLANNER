import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle
import tensorflow as tf
from tensorflow.keras.models import load_model
from typing import Tuple, Dict
import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# PHASE 1: DATA QUALITY TESTING
# ============================================================================

class DataQualityTests:
    """Test data integrity before training"""
    
    @staticmethod
    def test_data_completeness(X: np.ndarray, y: np.ndarray) -> Dict:
        """Check for missing values and data integrity"""
        results = {}
        
        # Check for NaNs
        X_has_nan = np.isnan(X).any()
        y_has_nan = np.isnan(y).any()
        
        results['X_has_nan'] = X_has_nan
        results['y_has_nan'] = y_has_nan
        results['X_nan_count'] = np.isnan(X).sum()
        results['y_nan_count'] = np.isnan(y).sum()
        
        # Check for infinities
        results['X_has_inf'] = np.isinf(X).any()
        results['y_has_inf'] = np.isinf(y).any()
        
        return results
    
    @staticmethod
    def test_data_ranges(X: np.ndarray, y: np.ndarray, 
                        expected_min=0, expected_max=1, auto_fix=True) -> Dict:
        """Verify data is within expected ranges"""
        results = {}
        
        results['X_min'] = float(X.min())
        results['X_max'] = float(X.max())
        results['y_min'] = float(y.min())
        results['y_max'] = float(y.max())
        
        X_in_range = (X.min() >= expected_min and X.max() <= expected_max)
        y_in_range = (y.min() >= expected_min and y.max() <= expected_max)
        
        results['X_in_range'] = X_in_range
        results['y_in_range'] = y_in_range
        results['auto_fix_applied'] = False
        results['values_clipped'] = 0
        
        # Auto-fix: Clip values to [0, 1] if they're slightly out of range
        if auto_fix and (not X_in_range or not y_in_range):
            # Only auto-fix if values are slightly out of range (likely floating point errors)
            if (X.min() >= -0.1 and X.max() <= 1.1 and 
                y.min() >= -0.1 and y.max() <= 1.1):
                values_clipped = ((X < 0) | (X > 1)).sum() + ((y < 0) | (y > 1)).sum()
                X[:] = np.clip(X, 0, 1)
                y[:] = np.clip(y, 0, 1)
                results['auto_fix_applied'] = True
                results['values_clipped'] = values_clipped
                results['X_in_range'] = True
                results['y_in_range'] = True
        
        return results
    
    @staticmethod
    def test_shapes(X_train, X_val, X_test, y_train, y_val, y_test,
                   expected_seq_len=12, expected_horizon=12, 
                   expected_sensors=325) -> Dict:
        """Verify all shapes are correct"""
        results = {}
        
        # Expected shapes
        results['X_train_shape'] = X_train.shape
        results['X_val_shape'] = X_val.shape
        results['X_test_shape'] = X_test.shape
        results['y_train_shape'] = y_train.shape
        results['y_val_shape'] = y_val.shape
        results['y_test_shape'] = y_test.shape
        
        # Validation
        results['seq_len_correct'] = (X_train.shape[1] == expected_seq_len)
        results['horizon_correct'] = (y_train.shape[1] == expected_horizon)
        results['sensors_correct'] = (X_train.shape[2] == expected_sensors)
        results['batch_dims_match'] = (X_train.shape[0] == y_train.shape[0])
        
        return results
    
    @staticmethod
    def test_data_distribution(X: np.ndarray, y: np.ndarray) -> Dict:
        """Check data statistics and distribution"""
        results = {}
        
        results['X_mean'] = float(X.mean())
        results['X_std'] = float(X.std())
        results['y_mean'] = float(y.mean())
        results['y_std'] = float(y.std())
        
        # Check for extreme outliers (> 3 std deviations)
        X_outliers = np.abs(X - X.mean()) > 3 * X.std()
        y_outliers = np.abs(y - y.mean()) > 3 * y.std()
        
        results['X_outlier_percentage'] = float((X_outliers.sum() / X.size) * 100)
        results['y_outlier_percentage'] = float((y_outliers.sum() / y.size) * 100)
        
        return results
    
    @staticmethod
    def run_all_tests(X_train, X_val, X_test, y_train, y_val, y_test) -> None:
        """Run all data quality tests and print report"""
        print("\n" + "="*70)
        print("PHASE 1: DATA QUALITY TESTING")
        print("="*70)
        
        # Test completeness
        print("\nâœ“ Testing Data Completeness...")
        completeness = DataQualityTests.test_data_completeness(X_train, y_train)
        assert not completeness['X_has_nan'], "âŒ NaNs found in X_train"
        assert not completeness['y_has_nan'], "âŒ NaNs found in y_train"
        print("  âœ… No NaN values found")
        
        # Test ranges with auto-fix
        print("\nâœ“ Testing Data Ranges...")
        ranges = DataQualityTests.test_data_ranges(X_train, y_train, auto_fix=True)
        
        if ranges['auto_fix_applied']:
            print(f"  âš ï¸  Auto-fixed out-of-range values: {ranges['values_clipped']} clipped")
            print(f"  X range (after fix): [{ranges['X_min']:.4f}, {ranges['X_max']:.4f}]")
            print(f"  y range (after fix): [{ranges['y_min']:.4f}, {ranges['y_max']:.4f}]")
            # Also fix validation and test sets
            DataQualityTests.test_data_ranges(X_val, y_val, auto_fix=True)
            DataQualityTests.test_data_ranges(X_test, y_test, auto_fix=True)
            print(f"  âœ… Applied same fix to validation and test sets")
        else:
            print(f"  X range: [{ranges['X_min']:.4f}, {ranges['X_max']:.4f}]")
            print(f"  y range: [{ranges['y_min']:.4f}, {ranges['y_max']:.4f}]")
        
        assert ranges['X_in_range'], "âŒ X data out of expected range"
        assert ranges['y_in_range'], "âŒ y data out of expected range"
        print("  âœ… Data in expected range")
        
        # Test shapes
        print("\nâœ“ Testing Data Shapes...")
        shapes = DataQualityTests.test_shapes(X_train, X_val, X_test, 
                                             y_train, y_val, y_test)
        assert shapes['seq_len_correct'], "âŒ Sequence length incorrect"
        assert shapes['horizon_correct'], "âŒ Horizon incorrect"
        assert shapes['sensors_correct'], "âŒ Sensor count incorrect"
        print(f"  X_train: {shapes['X_train_shape']}")
        print(f"  y_train: {shapes['y_train_shape']}")
        print(f"  X_val:   {shapes['X_val_shape']}")
        print(f"  X_test:  {shapes['X_test_shape']}")
        print("  âœ… All shapes correct")
        
        # Test distribution
        print("\nâœ“ Testing Data Distribution...")
        dist = DataQualityTests.test_data_distribution(X_train, y_train)
        print(f"  X mean: {dist['X_mean']:.4f} Â± {dist['X_std']:.4f}")
        print(f"  y mean: {dist['y_mean']:.4f} Â± {dist['y_std']:.4f}")
        print(f"  X outliers: {dist['X_outlier_percentage']:.2f}%")
        print(f"  y outliers: {dist['y_outlier_percentage']:.2f}%")
        if dist['X_outlier_percentage'] > 5:
            print("  âš ï¸  High outlier percentage in X")
        else:
            print("  âœ… Outlier percentage reasonable")
        
        print("\n" + "="*70)
        print("âœ… ALL DATA QUALITY TESTS PASSED")
        print("="*70)


# ============================================================================
# PHASE 3: PERFORMANCE METRICS EVALUATION
# ============================================================================

class PerformanceMetrics:
    """Calculate comprehensive performance metrics"""
    
    @staticmethod
    def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray, 
                         scaler=None, n_sensors: int = 325) -> Dict:
        """Calculate all relevant metrics
        
        Args:
            y_true: True values (can be 2D or 3D)
            y_pred: Predicted values (can be 2D or 3D)
            scaler: MinMaxScaler object (fitted on original data)
            n_sensors: Number of sensors/features in original data
        """
        # Work with flattened arrays for easier computation
        y_true_flat = y_true.reshape(-1)
        y_pred_flat = y_pred.reshape(-1)
        
        metrics = {}
        
        # Normalized space metrics (0-1 range)
        metrics['mse_normalized'] = float(mean_squared_error(y_true_flat, y_pred_flat))
        metrics['rmse_normalized'] = float(np.sqrt(metrics['mse_normalized']))
        metrics['mae_normalized'] = float(mean_absolute_error(y_true_flat, y_pred_flat))
        metrics['r2_normalized'] = float(r2_score(y_true_flat, y_pred_flat))
        
        # Denormalized metrics (actual speed values)
        if scaler is not None:
            # The scaler was fit on (timesteps, sensors) shaped data
            # We need to reshape our flat data back to (n_samples, n_sensors) format
            try:
                # Reshape flat data to proper 2D format for inverse_transform
                # The scaler expects shape (n_samples, n_features)
                # Since data was flattened, we reshape to (-1, n_sensors)
                y_true_reshaped = y_true_flat.reshape(-1, n_sensors)
                y_pred_reshaped = y_pred_flat.reshape(-1, n_sensors)
                
                # Inverse transform (scaler was fit on full 2D data)
                y_true_denorm = scaler.inverse_transform(y_true_reshaped)
                y_pred_denorm = scaler.inverse_transform(y_pred_reshaped)
                
                # Flatten back for metric calculation
                y_true_denorm = y_true_denorm.flatten()
                y_pred_denorm = y_pred_denorm.flatten()
                
            except (ValueError, AttributeError) as e:
                # Fallback: manual denormalization using scaler's min/max
                print(f"  âš ï¸  Using manual denormalization (reshape failed: {str(e)[:50]}...)")
                # Manual formula: y_denorm = y_norm * (max - min) + min
                # For multi-dimensional data, use the first feature's scale
                scale_factor = scaler.data_max_ - scaler.data_min_
                min_val = scaler.data_min_
                
                # Average scale across all sensors
                avg_scale = scale_factor.mean()
                avg_min = min_val.mean()
                
                y_true_denorm = y_true_flat * avg_scale + avg_min
                y_pred_denorm = y_pred_flat * avg_scale + avg_min
            
            metrics['mse_denorm'] = float(mean_squared_error(y_true_denorm, y_pred_denorm))
            metrics['rmse_denorm'] = float(np.sqrt(metrics['mse_denorm']))
            metrics['mae_denorm'] = float(mean_absolute_error(y_true_denorm, y_pred_denorm))
            metrics['r2_denorm'] = float(r2_score(y_true_denorm, y_pred_denorm))
            
            # MAPE (Mean Absolute Percentage Error)
            mask = y_true_denorm != 0
            if mask.sum() > 0:
                mape = np.mean(np.abs((y_true_denorm[mask] - y_pred_denorm[mask]) / 
                                      y_true_denorm[mask])) * 100
            else:
                mape = 0.0
            metrics['mape'] = float(mape)
            
            metrics['y_true_denorm_stats'] = {
                'min': float(y_true_denorm.min()),
                'max': float(y_true_denorm.max()),
                'mean': float(y_true_denorm.mean()),
                'std': float(y_true_denorm.std())
            }
            metrics['y_pred_denorm_stats'] = {
                'min': float(y_pred_denorm.min()),
                'max': float(y_pred_denorm.max()),
                'mean': float(y_pred_denorm.mean()),
                'std': float(y_pred_denorm.std())
            }
        
        return metrics
    
    @staticmethod
    def print_metrics_report(metrics: Dict, dataset_name: str = "Dataset") -> None:
        """Print formatted metrics report"""
        print(f"\n{'='*70}")
        print(f"PERFORMANCE METRICS - {dataset_name}")
        print(f"{'='*70}")
        
        print(f"\nðŸ“Š Normalized Metrics (0-1 range):")
        print(f"  MSE:  {metrics['mse_normalized']:.6f}")
        print(f"  RMSE: {metrics['rmse_normalized']:.6f}")
        print(f"  MAE:  {metrics['mae_normalized']:.6f}")
        print(f"  RÂ²:   {metrics['r2_normalized']:.6f}")
        
        if 'rmse_denorm' in metrics:
            print(f"\nðŸ“Š Denormalized Metrics (actual speed values in mph):")
            print(f"  MSE:  {metrics['mse_denorm']:.6f} mphÂ²")
            print(f"  RMSE: {metrics['rmse_denorm']:.4f} mph")
            print(f"  MAE:  {metrics['mae_denorm']:.4f} mph")
            print(f"  MAPE: {metrics['mape']:.2f}%")
            print(f"  RÂ²:   {metrics['r2_denorm']:.6f}")
            
            print(f"\nðŸ“ˆ True Values Statistics:")
            stats = metrics['y_true_denorm_stats']
            print(f"  Min:  {stats['min']:.2f} mph")
            print(f"  Max:  {stats['max']:.2f} mph")
            print(f"  Mean: {stats['mean']:.2f} mph")
            print(f"  Std:  {stats['std']:.2f} mph")
            
            print(f"\nðŸ”® Predicted Values Statistics:")
            stats = metrics['y_pred_denorm_stats']
            print(f"  Min:  {stats['min']:.2f} mph")
            print(f"  Max:  {stats['max']:.2f} mph")
            print(f"  Mean: {stats['mean']:.2f} mph")
            print(f"  Std:  {stats['std']:.2f} mph")


# ============================================================================
# PHASE 4: TEMPORAL VALIDATION
# ============================================================================

class TemporalValidation:
    """Time-series specific tests"""
    
    @staticmethod
    def test_temporal_consistency(y_pred: np.ndarray, 
                                 max_allowed_jump=0.15) -> Dict:
        """Check prediction smoothness over time"""
        results = {}
        
        # Flatten to time series
        y_pred_flat = y_pred.reshape(-1)
        
        # Calculate differences between consecutive time steps
        diffs = np.abs(np.diff(y_pred_flat))
        
        results['max_jump'] = float(diffs.max())
        results['mean_jump'] = float(diffs.mean())
        results['std_jump'] = float(diffs.std())
        results['within_threshold'] = (diffs.max() <= max_allowed_jump)
        results['jump_violations'] = int((diffs > max_allowed_jump).sum())
        
        return results
    
    @staticmethod
    def test_boundary_predictions(y_pred: np.ndarray, 
                                 expected_min=0, expected_max=1) -> Dict:
        """Ensure predictions stay within valid bounds"""
        results = {}
        
        results['pred_min'] = float(y_pred.min())
        results['pred_max'] = float(y_pred.max())
        results['within_bounds'] = ((y_pred >= expected_min).all() and 
                                   (y_pred <= expected_max).all())
        results['below_min'] = (y_pred < expected_min).sum()
        results['above_max'] = (y_pred > expected_max).sum()
        
        return results
    
    @staticmethod
    def run_temporal_tests(y_pred: np.ndarray) -> None:
        """Run all temporal tests"""
        print("\n" + "="*70)
        print("PHASE 4: TEMPORAL VALIDATION")
        print("="*70)
        
        # Test consistency
        print("\nâœ“ Testing Temporal Consistency...")
        consistency = TemporalValidation.test_temporal_consistency(y_pred)
        print(f"  Max jump between steps: {consistency['max_jump']:.6f}")
        print(f"  Mean jump: {consistency['mean_jump']:.6f}")
        print(f"  Violations: {consistency['jump_violations']}")
        if consistency['within_threshold']:
            print("  âœ… Predictions temporally smooth")
        else:
            print("  âš ï¸  Some large jumps detected")
        
        # Test boundaries
        print("\nâœ“ Testing Boundary Predictions...")
        bounds = TemporalValidation.test_boundary_predictions(y_pred)
        print(f"  Min prediction: {bounds['pred_min']:.6f}")
        print(f"  Max prediction: {bounds['pred_max']:.6f}")
        print(f"  Below min: {bounds['below_min']}")
        print(f"  Above max: {bounds['above_max']}")
        if bounds['within_bounds']:
            print("  âœ… All predictions within bounds")
        else:
            print("  âŒ Out-of-bounds predictions detected")
        
        print("\n" + "="*70)


# ============================================================================
# PHASE 5: EDGE CASE TESTING
# ============================================================================

class EdgeCaseTests:
    """Test model behavior on edge cases"""
    
    @staticmethod
    def test_extreme_values(model, X_test: np.ndarray, 
                           scaler=None) -> Dict:
        """Test on extreme traffic conditions"""
        results = {}
        
        # Create test samples with extreme values
        X_min = X_test.min(axis=(0, 2), keepdims=True)
        X_max = X_test.max(axis=(0, 2), keepdims=True)
        
        # Samples with minimum values (congestion)
        X_extreme_min = np.full_like(X_test[:10], X_min)
        pred_extreme_min = model.predict(X_extreme_min, verbose=0)
        results['pred_on_min_input'] = {
            'min': float(pred_extreme_min.min()),
            'max': float(pred_extreme_min.max()),
            'mean': float(pred_extreme_min.mean())
        }
        
        # Samples with maximum values (free flow)
        X_extreme_max = np.full_like(X_test[:10], X_max)
        pred_extreme_max = model.predict(X_extreme_max, verbose=0)
        results['pred_on_max_input'] = {
            'min': float(pred_extreme_max.min()),
            'max': float(pred_extreme_max.max()),
            'mean': float(pred_extreme_max.mean())
        }
        
        # Check if predictions make sense
        results['min_input_pred_lower'] = (pred_extreme_min.mean() < 
                                          pred_extreme_max.mean())
        
        return results
    
    @staticmethod
    def test_corrupted_input(model, X_test: np.ndarray, 
                            sensor_id=50) -> Dict:
        """Test with missing sensor data"""
        results = {}
        
        # Corrupt one sensor with mean value
        X_corrupted = X_test.copy()
        X_corrupted[:, :, sensor_id] = 0.5  # Replace with mean
        
        pred_clean = model.predict(X_test[:10], verbose=0)
        pred_corrupted = model.predict(X_corrupted[:10], verbose=0)
        
        # Check difference
        pred_diff = np.abs(pred_corrupted - pred_clean).mean()
        
        results['sensor_corrupted'] = sensor_id
        results['avg_prediction_change'] = float(pred_diff)
        results['predictions_still_valid'] = (
            (pred_corrupted >= 0).all() and (pred_corrupted <= 1).all()
        )
        
        return results
    
    @staticmethod
    def run_edge_case_tests(model, X_test: np.ndarray, 
                           scaler=None) -> None:
        """Run all edge case tests"""
        print("\n" + "="*70)
        print("PHASE 5: EDGE CASE TESTING")
        print("="*70)
        
        print("\nâœ“ Testing Extreme Values...")
        extreme = EdgeCaseTests.test_extreme_values(model, X_test, scaler)
        print(f"  Congested traffic predictions:")
        print(f"    Mean: {extreme['pred_on_min_input']['mean']:.6f}")
        print(f"  Free flow predictions:")
        print(f"    Mean: {extreme['pred_on_max_input']['mean']:.6f}")
        if extreme['min_input_pred_lower']:
            print("  âœ… Model correctly predicts lower speeds for congestion")
        
        print("\nâœ“ Testing Corrupted Input...")
        corrupted = EdgeCaseTests.test_corrupted_input(model, X_test)
        print(f"  Sensor {corrupted['sensor_corrupted']} corruption:")
        print(f"  Avg prediction change: {corrupted['avg_prediction_change']:.6f}")
        if corrupted['predictions_still_valid']:
            print("  âœ… Model handles corrupted input gracefully")
        
        print("\n" + "="*70)


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def plot_predictions_vs_actual(y_true: np.ndarray, y_pred: np.ndarray, 
                              sample_idx=0, timesteps=20):
    """Visualize predictions"""
    plt.figure(figsize=(14, 5))
    
    # Plot 1: Time series comparison
    plt.subplot(1, 2, 1)
    true_series = y_true[sample_idx, :timesteps, 0]
    pred_series = y_pred[sample_idx, :timesteps, 0]
    
    plt.plot(true_series, 'b-o', label='Actual', linewidth=2)
    plt.plot(pred_series, 'r--s', label='Predicted', linewidth=2)
    plt.xlabel('Time Steps')
    plt.ylabel('Normalized Speed')
    plt.title('Predictions vs Actual (Sample Series)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Plot 2: Scatter plot
    plt.subplot(1, 2, 2)
    plt.scatter(y_true.flatten(), y_pred.flatten(), alpha=0.3, s=1)
    
    # Perfect prediction line
    min_val = min(y_true.min(), y_pred.min())
    max_val = max(y_true.max(), y_pred.max())
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')
    
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.title('Prediction Scatter Plot')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def run_complete_testing_suite(model, X_train, X_val, X_test, 
                              y_train, y_val, y_test, 
                              scaler=None, n_sensors=325) -> None:
    """Run all testing phases
    
    Args:
        model: Trained CNN model
        X_train, X_val, X_test: Input data
        y_train, y_val, y_test: Target data
        scaler: Fitted MinMaxScaler
        n_sensors: Number of sensors/features (default 325 for PEMS Bay)
    """
    print("\n" + "#"*70)
    print("#" + " "*68 + "#")
    print("#" + "  CNN MODEL COMPREHENSIVE TESTING SUITE".center(68) + "#")
    print("#" + " "*68 + "#")
    print("#"*70)
    
    # Phase 1: Data Quality
    DataQualityTests.run_all_tests(X_train, X_val, X_test, 
                                   y_train, y_val, y_test)
    
    # Phase 3: Performance Metrics
    print("\n" + "="*70)
    print("PHASE 3: PERFORMANCE METRICS EVALUATION")
    print("="*70)
    
    # Test set predictions
    y_pred_test = model.predict(X_test, verbose=0)
    metrics_test = PerformanceMetrics.calculate_metrics(y_test, y_pred_test, scaler, n_sensors)
    PerformanceMetrics.print_metrics_report(metrics_test, "TEST SET")
    
    # Validation set predictions
    y_pred_val = model.predict(X_val, verbose=0)
    metrics_val = PerformanceMetrics.calculate_metrics(y_val, y_pred_val, scaler, n_sensors)
    PerformanceMetrics.print_metrics_report(metrics_val, "VALIDATION SET")
    
    # Phase 4: Temporal Validation
    TemporalValidation.run_temporal_tests(y_pred_test)
    
    # Phase 5: Edge Cases
    EdgeCaseTests.run_edge_case_tests(model, X_test, scaler)
    
    # Final Summary
    print("\n" + "#"*70)
    print("#" + " "*68 + "#")
    print("#" + "âœ… TESTING SUITE COMPLETED âœ…".center(68) + "#")
    print("#" + " "*68 + "#")
    print("#"*70)
    
    print("\nðŸ“Š KEY FINDINGS:")
    print(f"  â€¢ Test RMSE: {metrics_test['rmse_denorm']:.4f} mph")
    print(f"  â€¢ Test MAE: {metrics_test['mae_denorm']:.4f} mph")
    print(f"  â€¢ Test MAPE: {metrics_test['mape']:.2f}%")
    print(f"  â€¢ Test RÂ²: {metrics_test['r2_denorm']:.6f}")
    print(f"\n  â€¢ Overfitting gap (Val-Train RMSE): TBD (needs train predictions)")
    print(f"  â€¢ Model ready for production: Check against success criteria âœ“")
